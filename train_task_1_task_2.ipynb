{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7884,"status":"ok","timestamp":1736604632280,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"},"user_tz":-60},"id":"qnbPjo-SaCY7","outputId":"e7566c32-0cca-4553-ff59-90653978316b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting laplace-torch\n","  Downloading laplace_torch-0.2.2.2-py3-none-any.whl.metadata (5.1 kB)\n","Collecting asdfghjkl==0.1a4 (from laplace-torch)\n","  Downloading asdfghjkl-0.1a4-py3-none-any.whl.metadata (3.2 kB)\n","Collecting backpack-for-pytorch (from laplace-torch)\n","  Downloading backpack_for_pytorch-1.7.1-py3-none-any.whl.metadata (4.4 kB)\n","Collecting curvlinops-for-pytorch>=2.0 (from laplace-torch)\n","  Downloading curvlinops_for_pytorch-2.0.1-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from laplace-torch) (1.26.4)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (from laplace-torch) (3.4.0)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from laplace-torch) (2.5.1+cu121)\n","Collecting torchmetrics (from laplace-torch)\n","  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: torchvision>=0.15 in /usr/local/lib/python3.10/dist-packages (from laplace-torch) (0.20.1+cu121)\n","Requirement already satisfied: scipy<2.0.0,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from curvlinops-for-pytorch>=2.0->laplace-torch) (1.13.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.61.0 in /usr/local/lib/python3.10/dist-packages (from curvlinops-for-pytorch>=2.0->laplace-torch) (4.67.1)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from curvlinops-for-pytorch>=2.0->laplace-torch) (0.8.0)\n","Collecting einconv (from curvlinops-for-pytorch>=2.0->laplace-torch)\n","  Downloading einconv-0.1.0-py3-none-any.whl.metadata (1.9 kB)\n","Collecting unfoldNd<1.0.0,>=0.2.0 (from backpack-for-pytorch->laplace-torch)\n","  Downloading unfoldNd-0.2.3-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->laplace-torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->laplace-torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->laplace-torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->laplace-torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->laplace-torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->laplace-torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->laplace-torch) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.15->laplace-torch) (11.1.0)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics->laplace-torch) (24.2)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics->laplace-torch)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics->laplace-torch) (75.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->laplace-torch) (3.0.2)\n","Downloading laplace_torch-0.2.2.2-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading asdfghjkl-0.1a4-py3-none-any.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading curvlinops_for_pytorch-2.0.1-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backpack_for_pytorch-1.7.1-py3-none-any.whl (196 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.6/196.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Downloading unfoldNd-0.2.3-py3-none-any.whl (16 kB)\n","Downloading einconv-0.1.0-py3-none-any.whl (27 kB)\n","Installing collected packages: lightning-utilities, unfoldNd, torchmetrics, einconv, asdfghjkl, backpack-for-pytorch, curvlinops-for-pytorch, laplace-torch\n","Successfully installed asdfghjkl-0.1a4 backpack-for-pytorch-1.7.1 curvlinops-for-pytorch-2.0.1 einconv-0.1.0 laplace-torch-0.2.2.2 lightning-utilities-0.11.9 torchmetrics-1.6.1 unfoldNd-0.2.3\n"]}],"source":["!pip install laplace-torch"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XGyqXJhjAZ-Z","executionInfo":{"status":"ok","timestamp":1736604643843,"user_tz":-60,"elapsed":11565,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}}},"outputs":[],"source":["import random\n","import numpy as np\n","import os\n","import sys\n","import torch\n","from torchvision.transforms import transforms\n","from torchvision import datasets\n","from collections import Counter\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","from torch.utils.data import DataLoader, Subset, ConcatDataset, random_split\n","import numpy as np\n","from copy import deepcopy\n","import numpy as np\n","from laplace import Laplace\n","from laplace.curvature import AsdlGGN\n","import torch\n","import torch.nn.functional as F\n","import pickle\n","import time"]},{"cell_type":"code","source":["# Check if running on Colab\n","try:\n","  import google.colab\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  from google.colab import drive\n","  # Connect to Google drive where the training data is located\n","  drive.mount(\"/content/gdrive\")\n","  work_dir = \"/content/gdrive/My Drive/Colab Notebooks/DL-Project-2024-Experiments/SUBMISSION\"\n","  os.chdir(work_dir)\n","  print(f\"Connected to Google drive, setting working directory to '{work_dir}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQh2TBhVsw3U","executionInfo":{"status":"ok","timestamp":1736604675096,"user_tz":-60,"elapsed":31263,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}},"outputId":"6096a560-77ff-4759-c78e-614b27e88908"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","Connected to Google drive, setting working directory to '/content/gdrive/My Drive/Colab Notebooks/DL-Project-2024-Experiments/SUBMISSION'\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1736604675096,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"},"user_tz":-60},"id":"yi_1C7xsAE4E","outputId":"ee664e3f-d795-43df-ad93-dc9e8b931c3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of available GPUs: 1\n","Device: cuda\n","Number of CPUs: 12\n","System version: sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\n"]}],"source":["# Check if GPUs are available\n","device=torch.device(\"cpu\")\n","if torch.cuda.is_available():\n","    device=torch.device(\"cuda\")\n","    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n","print(f\"Device: {device}\")\n","\n","# Check system\n","print(f\"Number of CPUs: {os.cpu_count()}\")\n","print(f\"System version: {sys.version_info}\")"]},{"cell_type":"code","source":["# Create results folder if it does not exist yet\n","results_folder_name = 'raw_result_data'\n","if not os.path.exists(results_folder_name):\n","    os.makedirs(results_folder_name)"],"metadata":{"id":"E5y5h4x8NPJf","executionInfo":{"status":"ok","timestamp":1736604675496,"user_tz":-60,"elapsed":407,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MQTQNAgaZU1K","executionInfo":{"status":"ok","timestamp":1736604675497,"user_tz":-60,"elapsed":7,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}}},"outputs":[],"source":["EPOCHS=50\n","BATCH_SIZE=100\n","LEARNING_RATE=0.001\n","OPTIMIZER_MOMENTUM=0.9\n","N_CLASSES=10\n","MEASURE_SENSITIVITY_ON_LAST_EPOCH_ONLY=True"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24690,"status":"ok","timestamp":1736604700181,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"},"user_tz":-60},"id":"47t1-s0SCxcl","outputId":"b518c185-7bee-46b7-d5c1-501a37cb8721"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Loaded CIFAR10 data: training=50000 items, testing=10000 items.\n"]}],"source":["# Load training data\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","trainset = datasets.CIFAR10(root='./data', train=True,\n","                            download=True,\n","                            transform=transform)\n","testset = datasets.CIFAR10(root='./data', train=False,\n","                           download=True,\n","                           transform=transform)\n","\n","print(f\"Loaded CIFAR10 data: training={len(trainset)} items, testing={len(testset)} items.\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yaEFLmXSJuVb","executionInfo":{"status":"ok","timestamp":1736604700182,"user_tz":-60,"elapsed":9,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}}},"outputs":[],"source":["def split_dataset_to_labels(dataset, subset_labels):\n","  return Subset(dataset, [i for i,label in enumerate(dataset.targets) if label in subset_labels])\n","\n","subset_A_labels=[0,1,2,3,4]\n","subset_B_labels=[5,6,7,8,9]\n","\n","# split the CIFAR10 data into two subsets according to their labels\n","trainset_A=split_dataset_to_labels(trainset, subset_A_labels)\n","trainset_B=split_dataset_to_labels(trainset, subset_B_labels)\n","\n","testset_A=split_dataset_to_labels(testset, subset_A_labels)\n","testset_B=split_dataset_to_labels(testset, subset_B_labels)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Unn8_PoPJfnu","executionInfo":{"status":"ok","timestamp":1736604700182,"user_tz":-60,"elapsed":7,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}}},"outputs":[],"source":["def check_class_balance(dataset):\n","  label_counter = Counter()\n","  for _, label in dataset:\n","    label_counter[label]+=1\n","  total=len(dataset)\n","  for label in sorted(label_counter.keys()):\n","    n_label=label_counter[label]\n","    percentage=100*(n_label/total)\n","    print(f\"Label {label}: {label_counter[label]}/{len(dataset)}={percentage:.0f}%, \")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12019,"status":"ok","timestamp":1736604712194,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"},"user_tz":-60},"id":"8KbnH-wDJTk4","outputId":"34eaac83-4fdb-40fa-895c-8197579a89b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label 0: 5000/50000=10%, \n","Label 1: 5000/50000=10%, \n","Label 2: 5000/50000=10%, \n","Label 3: 5000/50000=10%, \n","Label 4: 5000/50000=10%, \n","Label 5: 5000/50000=10%, \n","Label 6: 5000/50000=10%, \n","Label 7: 5000/50000=10%, \n","Label 8: 5000/50000=10%, \n","Label 9: 5000/50000=10%, \n","None\n"]}],"source":["#check if labels are balanced\n","print(check_class_balance(trainset))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"97-eodudQirG","executionInfo":{"status":"ok","timestamp":1736604712194,"user_tz":-60,"elapsed":12,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}}},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"G_yBkkP5I5S4","executionInfo":{"status":"ok","timestamp":1736604712194,"user_tz":-60,"elapsed":10,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}}},"outputs":[],"source":["def softmax_hessian(probs, eps=1e-10):\n","  \"\"\"copied from https://github.com/team-approx-bayes/memory-perturbation\n","  \"\"\"\n","  return torch.clamp(probs - probs * probs, min=eps, max=1 - eps)\n","\n","def get_pred_vars_laplace(net, trainloader, nc, version='kfac', device='cuda'):\n","  \"\"\"copied from https://github.com/team-approx-bayes/memory-perturbation\n","  \"\"\"\n","  if device=='cuda':\n","      torch.cuda.empty_cache()\n","  elif device=='mps':\n","      torch.mps.empty_cache()\n","\n","\n","  if device == 'mps':\n","      # use simplyfied approximation on local since kfac times out\n","      version = 'diag'\n","\n","  if version == 'kfac':\n","      hessian_structure = 'kron'\n","  elif version == 'diag':\n","      hessian_structure = 'diag'\n","\n","\n","  laplace_object = Laplace(\n","      net, 'classification',\n","      subset_of_weights='all',\n","      hessian_structure=hessian_structure,\n","      backend=AsdlGGN,\n","      )\n","\n","  if device=='cuda':\n","      torch.cuda.empty_cache()\n","  elif device=='mps':\n","      torch.mps.empty_cache()\n","\n","  laplace_object.fit(trainloader)\n","\n","  fvars = np.empty(shape=(0, nc))\n","  for inputs, _ in trainloader:\n","      inputs = inputs.to(device)\n","      _, fvar = laplace_object._glm_predictive_distribution(inputs)\n","      fvars = np.vstack((fvars, np.diagonal(fvar.cpu().numpy(), axis1=1, axis2=2)))\n","\n","  del laplace_object\n","  if device=='cuda':\n","      torch.cuda.empty_cache()\n","  elif device=='mps':\n","      torch.mps.empty_cache()\n","  return fvars.tolist()\n","\n","def prediction_sensitivity(model, var_dataloader, n_classes, device):\n","  residuals_list = []\n","  lams_list = []\n","\n","  for batch, (X, y) in enumerate(var_dataloader, 0):\n","      X, y = X.to(device), y.to(device)\n","      with torch.no_grad():\n","          model.eval()\n","          pred = model(X)\n","          probs = F.softmax(pred, dim=-1)\n","          residuals_list.append((probs - F.one_hot(y, n_classes)).detach().cpu().numpy())\n","          lams = softmax_hessian(probs).cpu().numpy()\n","          lams_list.append(lams)\n","\n","  lambdas, residuals = np.vstack(lams_list), np.vstack(residuals_list)\n","\n","  print('tracking variances...')\n","  model.train()\n","  vars = get_pred_vars_laplace(model, var_dataloader, n_classes, device=device)\n","  print('done')\n","\n","  sensitivities = np.asarray(residuals) * np.asarray(lambdas) * np.asarray(vars)\n","  sensitivities = np.sum(np.abs(sensitivities), axis=-1)\n","\n","  return sensitivities\n","\n","def test_loop(test_loader, model, loss_fn, device):\n","  model.eval()\n","  running_loss = 0.0\n","  n_correct=0.0\n","  for X,y in test_loader:\n","    X, y = X.to(device), y.to(device)\n","    pred = model(X)\n","    loss = loss_fn(pred, y)\n","    running_loss+=loss.item()\n","    n_correct+=(pred.argmax(1) == y).type(torch.float).sum().item()\n","  accuracy = n_correct/len(test_loader.dataset)\n","  return running_loss, accuracy\n","\n","def task1(trainset, test_set_1, test_set_2, loss_fn, n_classes, n_epochs, device):\n","  dataloader=DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=False)\n","  dataloader_var=DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","  test_loader_1 = DataLoader(test_set_1, batch_size=BATCH_SIZE, shuffle=True)\n","  test_loader_2 = DataLoader(test_set_2, batch_size=BATCH_SIZE, shuffle=True)\n","  model = CNN()\n","  model = model.to(device)\n","  model.train()\n","  optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=OPTIMIZER_MOMENTUM)\n","  cp_epochs_list=[]\n","  test_accuracies_1 = []\n","  test_accuracies_2 = []\n","  sensitivities_epochs_list = []\n","  for epoch in range(n_epochs):\n","    cp_batches_list=[]\n","    running_loss=0.0\n","    #training loop\n","    for batch, (X, y) in enumerate(dataloader, 0):\n","      X, y = X.to(device), y.to(device)\n","      optimizer.zero_grad()\n","      pred = model(X)\n","      loss = loss_fn(pred, y)\n","      loss.backward()\n","      optimizer.step()\n","      running_loss+=loss.item()\n","      with torch.no_grad():\n","        probs = F.softmax(pred, dim=-1)\n","        _, preds = probs.max(1)\n","        cp_batch=(preds==y).detach().cpu().numpy()\n","        cp_batches_list.append(cp_batch)\n","\n","    with torch.no_grad():\n","      _, test_1_accuracy = test_loop(test_loader_1, model, loss_fn, device)\n","      test_accuracies_1.append(test_1_accuracy)\n","      _, test_2_accuracy = test_loop(test_loader_2, model, loss_fn, device)\n","      test_accuracies_2.append(test_2_accuracy)\n","    print(f\"Task 1/2: Epoch [{epoch+1}/{n_epochs}]: test_1_accuracy={test_1_accuracy:.2%},  test_2_accuracy={test_2_accuracy:.2%}\")\n","\n","    running_loss = 0.0\n","    cp_epoch=np.hstack(cp_batches_list)\n","    cp_epochs_list.append(cp_epoch)\n","    if not MEASURE_SENSITIVITY_ON_LAST_EPOCH_ONLY:\n","      # calculate sensitivity\n","      sensitivities_epoch = prediction_sensitivity(model, dataloader_var, n_classes, device)\n","      sensitivities_epochs_list.append(sensitivities_epoch)\n","\n","  if MEASURE_SENSITIVITY_ON_LAST_EPOCH_ONLY:\n","    # calculate sensitivity\n","    sensitivities_epoch = prediction_sensitivity(model, dataloader_var, n_classes, device)\n","    sensitivities_epochs_list.append(sensitivities_epoch)\n","\n","  correct_predictions=np.stack(cp_epochs_list, axis=-1)\n","  learning_speeds=np.mean(correct_predictions.astype(int), axis=-1)\n","\n","  sensitivities_experiment=np.stack(sensitivities_epochs_list, axis=0)\n","  sensitivities=np.mean(sensitivities_experiment, axis=0)\n","  return sensitivities, learning_speeds, test_accuracies_1, test_accuracies_2, model\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"3aBL3LMyJuVd","executionInfo":{"status":"ok","timestamp":1736604712195,"user_tz":-60,"elapsed":10,"user":{"displayName":"Philipp Richter","userId":"16402186915618900674"}}},"outputs":[],"source":["def train_loop(train_loader, model, loss_fn, optimizer, device):\n","  model.train()\n","  running_loss = 0.0\n","  n_correct=0.0\n","  for X,y in train_loader:\n","    X, y = X.to(device), y.to(device)\n","    optimizer.zero_grad()\n","    pred = model(X)\n","    loss = loss_fn(pred, y)\n","    loss.backward()\n","    optimizer.step()\n","    running_loss+=loss.item()\n","    n_correct+=(pred.argmax(1) == y).type(torch.float).sum().item()\n","  accuracy = n_correct/len(train_loader.dataset)\n","  return running_loss, accuracy\n","\n","def task2(model, train_set_2, buffer_set, test_set_1, test_set_2, n_epochs, learning_rate, optimizer_momentum, batch_size, verbose=True):\n","\n","  test_accuracies_1 = []\n","  test_accuracies_2 = []\n","\n","  train_loader_2_with_buffer = DataLoader(ConcatDataset([train_set_2, buffer_set]), batch_size=batch_size, shuffle=True)\n","\n","  test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, shuffle=True)\n","  test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, shuffle=True)\n","  model = model.to(device)\n","\n","  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=optimizer_momentum)\n","  loss_fn = nn.CrossEntropyLoss()\n","\n","  for epoch in range(n_epochs):\n","\n","    train_loop(train_loader_2_with_buffer, model, loss_fn, optimizer, device)\n","\n","    _, test_1_accuracy = test_loop(test_loader_1, model, loss_fn, device)\n","    test_accuracies_1.append(test_1_accuracy)\n","    _, test_2_accuracy = test_loop(test_loader_2, model, loss_fn, device)\n","    test_accuracies_2.append(test_2_accuracy)\n","\n","    if verbose:\n","      print(f\"Task 2/2: Epoch [{epoch+1}/{n_epochs}]: test_1_accuracy={test_1_accuracy:.2%},  test_2_accuracy={test_2_accuracy:.2%}\")\n","\n","  test_accuracies_1=np.stack(test_accuracies_1, axis=0)\n","  test_accuracies_2=np.stack(test_accuracies_2, axis=0)\n","\n","  return test_accuracies_1, test_accuracies_2\n","\n","def get_random_buffer(dataset, n_samples):\n","  rng=np.random.default_rng()\n","  buffer_set=Subset(dataset, rng.choice(len(dataset),n_samples, replace=False))\n","  return buffer_set\n","\n","def get_goldilocks_buffer(dataset, n_samples, learning_speeds, remove_lowest_pct=0.0, remove_highest_pct=0.0):\n","  learning_speeds_ranking=np.argsort(learning_speeds)\n","  rng=np.random.default_rng()\n","  buffer_set=Subset(dataset, rng.choice(learning_speeds_ranking[int(len(learning_speeds)*remove_lowest_pct):int(len(learning_speeds)*(1.0-remove_highest_pct))],n_samples, replace=False))\n","  return buffer_set\n","\n","def get_sensitivity_buffer(dataset, n_samples, sensitivities, highest):\n","  sensitivities_ranking=np.argsort(sensitivities)\n","  if highest:\n","    sensitivities_ranking=sensitivities_ranking[::-1]\n","  buffer_set=Subset(dataset, sensitivities_ranking[0:n_samples])\n","  return buffer_set\n","\n","def get_mixed_buffer(dataset, size_pct, goldilocks_rate=0.0, mpe_rate=0.0, learning_speeds=None, sensitivities=None, goldilocks_remove_lowest_pct=0.0, goldilocks_remove_highest_pct=0.0, mpe_highest=False):\n","  n_samples_total=int(len(dataset)*size_pct)\n","  n_samples_goldilocks=0\n","  n_samples_mpe=0\n","  n_samples_random=0\n","\n","  # This logic is to avoid being off by 1 sample due to rounding\n","  if goldilocks_rate==0.0:\n","    n_samples_mpe=int(n_samples_total*mpe_rate)\n","    n_samples_random=n_samples_total-n_samples_mpe\n","  elif mpe_rate==0:\n","    n_samples_goldilocks=int(n_samples_total*goldilocks_rate)\n","    n_samples_random=n_samples_total-n_samples_goldilocks\n","  elif goldilocks_rate+mpe_rate==1.0:\n","    n_samples_goldilocks=int(n_samples_total*goldilocks_rate)\n","    n_samples_mpe=n_samples_total-n_samples_goldilocks\n","  else:\n","    n_samples_goldilocks=int(n_samples_total*goldilocks_rate)\n","    n_samples_mpe=int(n_samples_total*mpe_rate)\n","    n_samples_random=n_samples_total-n_samples_goldilocks-n_samples_mpe\n","\n","  mix=[]\n","\n","  if n_samples_goldilocks>0:\n","    b = get_goldilocks_buffer(dataset, n_samples_goldilocks, learning_speeds, remove_lowest_pct=goldilocks_remove_lowest_pct, remove_highest_pct=goldilocks_remove_highest_pct)\n","    mix.append(b)\n","  if n_samples_mpe>0:\n","    b = get_sensitivity_buffer(dataset, n_samples_mpe, sensitivities, mpe_highest)\n","    mix.append(b)\n","  if n_samples_random>0:\n","    b = get_random_buffer(dataset, n_samples_random)\n","    mix.append(b)\n","\n","  buffer_set=Subset(dataset, [])\n","  if len(mix)>0:\n","    buffer_set = ConcatDataset(mix)\n","\n","  return buffer_set\n","\n","def get_file_key(n_epochs, experiment, buffer_pct, goldilocks_rate, mpe_rate, goldilocks_remove_lowest_pct, goldilocks_remove_highest_pct, mpe_highest):\n","  return f'e2e_ep_{n_epochs}_ex_{experiment}_b_{buffer_pct}_gl_{goldilocks_rate}_mpe_{mpe_rate}_gll_{goldilocks_remove_lowest_pct}_glh_{goldilocks_remove_highest_pct}_mpeh_{mpe_highest}'\n","\n","def task_2_orchestration(model, trainset_A, trainset_B, testset_A, testset_B, n_epochs, experiment, learning_rate, optimizer_momentum, batch_size, buffer_pct, test_1_accuracy, test_2_accuracy, goldilocks_rate=0.0, mpe_rate=0.0, learning_speeds=None, sensitivities=None, goldilocks_remove_lowest_pct=0.0, goldilocks_remove_highest_pct=0.0, mpe_highest=False, verbose=True):\n","  file_key=get_file_key(n_epochs, experiment, buffer_pct, goldilocks_rate, mpe_rate, goldilocks_remove_lowest_pct, goldilocks_remove_highest_pct, mpe_highest)\n","  filename=f\"{results_folder_name}/task_accuracies_data_{file_key}.pkl\"\n","\n","  buffer_set = get_mixed_buffer(trainset_A, buffer_pct, goldilocks_rate, mpe_rate, learning_speeds, sensitivities, goldilocks_remove_lowest_pct, goldilocks_remove_highest_pct, mpe_highest)\n","  test_1_accuracy_2, test_2_accuracy_2 = task2(model, trainset_B, buffer_set, testset_A, testset_B, n_epochs, learning_rate, optimizer_momentum, batch_size, verbose=True)\n","  description=f\"epochs={n_epochs}, exp={experiment}, lr={learning_rate}, bsize={buffer_pct:.0%} (gl={goldilocks_rate:.0%}, mpe={mpe_rate:.0%}, rnd={1.0-goldilocks_rate-mpe_rate:.0%})\\n\"\n","  if goldilocks_rate>0.0:\n","    description=description+f\", goldilocks: [remove_lowest_pct={goldilocks_remove_lowest_pct:.0%}, remove_highest_pct={goldilocks_remove_highest_pct:.0%}]\"\n","  if mpe_rate>0.0:\n","    description=description+f\", mpe: [highest={mpe_highest}]\"\n","\n","  # save the results to a file\n","  full_test_accuracies_1 = [*test_1_accuracy, *test_1_accuracy_2]\n","  full_test_accuracies_2 = [*test_2_accuracy, *test_2_accuracy_2]\n","\n","  all_results={\"n_epochs\":n_epochs,\"experiment\":experiment, \"learning_rate\":learning_rate,\"optimizer_momentum\": optimizer_momentum,\n","                \"batch_size\":batch_size,\"buffer_pct\":buffer_pct,\"goldilocks_rate\":goldilocks_rate, \"mpe_rate\":mpe_rate,\n","                \"goldilocks_remove_lowest_pct\":goldilocks_remove_lowest_pct,\"goldilocks_remove_highest_pct\": goldilocks_remove_highest_pct,\n","                \"mpe_highest\":mpe_highest, \"description\": description, \"test_accuracies_1\": full_test_accuracies_1, \"test_accuracies_2\": full_test_accuracies_2,\n","                \"learning_speeds\": learning_speeds, \"sensitivities\": sensitivities}\n","\n","  with open(filename, 'wb') as f:\n","    pickle.dump(all_results, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HjOWOAxJuVe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"67b6791f-8cd2-4552-8499-cfdb36dc26c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Task 1/2: Epoch [1/50]: test_1_accuracy=20.16%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [2/50]: test_1_accuracy=35.00%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [3/50]: test_1_accuracy=37.98%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [4/50]: test_1_accuracy=43.70%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [5/50]: test_1_accuracy=48.44%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [6/50]: test_1_accuracy=51.48%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [7/50]: test_1_accuracy=53.10%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [8/50]: test_1_accuracy=54.16%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [9/50]: test_1_accuracy=54.54%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [10/50]: test_1_accuracy=55.96%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [11/50]: test_1_accuracy=56.96%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [12/50]: test_1_accuracy=58.38%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [13/50]: test_1_accuracy=59.58%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [14/50]: test_1_accuracy=60.70%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [15/50]: test_1_accuracy=61.66%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [16/50]: test_1_accuracy=62.34%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [17/50]: test_1_accuracy=62.74%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [18/50]: test_1_accuracy=63.48%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [19/50]: test_1_accuracy=64.38%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [20/50]: test_1_accuracy=64.86%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [21/50]: test_1_accuracy=65.30%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [22/50]: test_1_accuracy=65.62%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [23/50]: test_1_accuracy=65.94%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [24/50]: test_1_accuracy=66.20%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [25/50]: test_1_accuracy=66.56%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [26/50]: test_1_accuracy=66.68%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [27/50]: test_1_accuracy=66.94%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [28/50]: test_1_accuracy=67.46%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [29/50]: test_1_accuracy=67.74%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [30/50]: test_1_accuracy=68.14%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [31/50]: test_1_accuracy=68.28%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [32/50]: test_1_accuracy=68.46%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [33/50]: test_1_accuracy=68.80%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [34/50]: test_1_accuracy=69.08%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [35/50]: test_1_accuracy=69.30%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [36/50]: test_1_accuracy=69.52%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [37/50]: test_1_accuracy=69.72%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [38/50]: test_1_accuracy=70.22%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [39/50]: test_1_accuracy=70.50%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [40/50]: test_1_accuracy=70.52%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [41/50]: test_1_accuracy=70.50%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [42/50]: test_1_accuracy=70.70%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [43/50]: test_1_accuracy=71.06%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [44/50]: test_1_accuracy=71.22%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [45/50]: test_1_accuracy=71.44%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [46/50]: test_1_accuracy=71.56%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [47/50]: test_1_accuracy=71.66%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [48/50]: test_1_accuracy=71.82%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [49/50]: test_1_accuracy=72.12%,  test_2_accuracy=0.00%\n","Task 1/2: Epoch [50/50]: test_1_accuracy=72.20%,  test_2_accuracy=0.00%\n","tracking variances...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n","/usr/local/lib/python3.10/dist-packages/laplace/baselaplace.py:1260: UserWarning: ASDL backend is used which does not support backprop through the functional variance, but `self.enable_backprop = True`. Falling back to using `self.backend.functorch_jacobians` which can be memory intensive for large models.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["done\n","Task 2/2: Epoch [1/50]: test_1_accuracy=0.00%,  test_2_accuracy=58.56%\n"]}],"source":["loss_fn = nn.CrossEntropyLoss()\n","sensitivities, learning_speed, test_1_accuracy, test_2_accuracy, model = task1(trainset_A, testset_A, testset_B, loss_fn, N_CLASSES, EPOCHS, device)\n","\n","base_parameters={\"trainset_A\": trainset_A, \"trainset_B\":trainset_B, \"testset_A\": testset_A, \"testset_B\":testset_B,\n","                    \"n_epochs\": EPOCHS, \"experiment\": int(time.time()), \"learning_rate\":LEARNING_RATE,\n","                    \"optimizer_momentum\": OPTIMIZER_MOMENTUM, \"batch_size\": BATCH_SIZE, \"verbose\": True,\n","                    \"learning_speeds\":learning_speed, \"sensitivities\":sensitivities,\n","                    \"test_1_accuracy\": test_1_accuracy, \"test_2_accuracy\": test_2_accuracy}\n","\n","# No buffer (baseline)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.00, **base_parameters)\n","\n","# Pure random buffers\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, **base_parameters)\n","\n","# Pure Goldilocks, 40%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, **base_parameters)\n","\n","# Pure Goldilocks, 20%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, **base_parameters)\n","\n","# Pure Goldilocks, 4%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=1.00, mpe_rate=0.00, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, **base_parameters)\n","\n","# Pure MPE, 40%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=0.00, mpe_rate=1.00, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=0.00, mpe_rate=1.00, mpe_highest=True, **base_parameters)\n","\n","# Pure MPE, 20%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=0.00, mpe_rate=1.00, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=0.00, mpe_rate=1.00, mpe_highest=True, **base_parameters)\n","\n","# Pure MPE, 4%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=0.00, mpe_rate=1.00, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=0.00, mpe_rate=1.00, mpe_highest=True, **base_parameters)\n","\n","# Mixed Goldilocks and MPE, 40%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, mpe_highest=False, **base_parameters)\n","\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, mpe_highest=True, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, mpe_highest=True, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.40, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, mpe_highest=True, **base_parameters)\n","\n","# Mixed Goldilocks and MPE, 20%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, mpe_highest=False, **base_parameters)\n","\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, mpe_highest=True, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, mpe_highest=True, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.20, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, mpe_highest=True, **base_parameters)\n","\n","# Mixed Goldilocks and MPE, 4%\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, mpe_highest=False, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, mpe_highest=False, **base_parameters)\n","\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.15, goldilocks_remove_highest_pct=0.45, mpe_highest=True, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.30, goldilocks_remove_highest_pct=0.30, mpe_highest=True, **base_parameters)\n","task_2_orchestration(deepcopy(model),buffer_pct=0.04, goldilocks_rate=0.50, mpe_rate=0.50, goldilocks_remove_lowest_pct=0.45, goldilocks_remove_highest_pct=0.15, mpe_highest=True, **base_parameters)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1dempNayKPfDaDFXsh2GftP0V5Am566Eg","timestamp":1736431812128},{"file_id":"1Hkf3sCDVEShain1kwy7l3Pz4mze0RzHs","timestamp":1735503730986},{"file_id":"1PTky3n8epYX4GigfiI4LmhWEfZF6vw8i","timestamp":1735488217645},{"file_id":"1SFtbyfhUlVI6GOOH3YcEV_zlv8k1AfGw","timestamp":1735469109109},{"file_id":"1F2EW4k16VHEQwexFzJqzo2UNWKGd_MsL","timestamp":1735415267284},{"file_id":"1Ftf9OUKmunotqDF5nwxz_TjN-Lg5xfEs","timestamp":1735415214679},{"file_id":"1L0qhMwhBUphlJndhLFGzjALTjZ_uvMu-","timestamp":1735413986878},{"file_id":"1rHgWEuaNpih5eOcBUP3afNfopiocQtjN","timestamp":1735402309927},{"file_id":"1sWWHbY1S2JeI7_u22zs58OgHr28-xZYV","timestamp":1735398107345},{"file_id":"1jwq4QMExHNhvuDGKY81X2A2WZZXg5HtW","timestamp":1735388056809}]},"kernelspec":{"display_name":"venv_dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"}},"nbformat":4,"nbformat_minor":0}