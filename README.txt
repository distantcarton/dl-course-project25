## Title
Project Report – Deep Learning class 2024-2025

## Project Description
This project analyzes the use of mixed Goldilocks/MPE buffers to mitigate the effect of catastrophic forgetting. The CIFAR10 dataset is used to analyze the evolution of the testing accuracy over all training epochs for a CNN where only the first 5 labels (task 1) are trained for the first half of the epochs, and then the other 5 labels (task 2) are trained for the second half of the epochs, while adding some samples from task 1 as an experience replay buffer, to try to make the network not fully forget what it has learned in task 1 training.

## Installation
Please create a new folder from where the notebooks can be run, and copy all the notebook files into this folder.
The use of a GPU instance supporting cuda is strongly recommended for the notebooks which perform any kind of training. We performed our training on Google Cloud instances using an A100 GPU (40 GB GPU RAM), 12 CPUs, 80 GB of system RAM and 235 GB available disk size.
The package “laplace-torch” is used by the sensitivity calculation logic, so it has to be installed prior to execution of the notebooks. All other packages used by the notebooks were available by default in our cloud instances, but might have to be installed if missing.

## Usage

# train_all.ipynb
First of all, in order to gauge the ability of the CNN to fit the CIFAR10 data, a plain-vanilla training and testing can be performed by executing train_all.ipynb. The notebook uses 100 epochs, batch size 100 (due to limited GPU RAM) and learning rate 0.001. This shows that the network is clearly starting to overfit above 50 epochs, so we chose to limit all further analysis to 50 epochs per training task.
The resulting training and testing accuracies are plotted, and the plot is saved as a PNG in an automatically created sub-folder “train_all_results” of the folder where the notebook has been executed.

# train_task_1_task_2.ipynb
This is the main notebook where the training is done, so GPU usage is strongly recommended. 
The notebook loads the CIFAR10 data and splits both the training and the testing data into 2 subsets with labels 0-4 (for task 1) and with labels 5-9 (for task 2) respectively.
The sensitivity is only measured at the end of task 1 training by default. However, setting MEASURE_SENSITIVITY_ON_LAST_EPOCH_ONLY=False will cause the notebook to measure the sensitivity after each epoch and average it over all epochs of the task 1 training.
The notebooks first calculates the learning speeds and the sensitivities using function task1(), which also yields a pre-trained model which is then deep-cloned and used for each subsequent experiment.
At the very end of the notebook, the experiments are started using the task_2_orchestration() function for each set of scenario parameters. This function requires the buffer size (buffer_pct), the fraction of the buffer used by Goldilocks samples (goldilocks_rate) and MPE samples (mpe_rate) respectively. If these two fractions do not add up to 1.0, the remainder will be filled with uniformly sampled samples, so a combination of goldilocks_rate=0.0 and mpe_rate=0.0 will lead to a pure random buffer. The Goldilocks buffer can be instructed to not use samples with high/low learning speed by setting the fraction which should be discarded from the top/bottom (goldilocks_remove_highest_pct/ goldilocks_remove_lowest_pct). The MPE buffer can be instructed to use the samples with the highest/lowest sensitivity by setting mpe_highest to True/False. For mixed buffers, both the Goldilocks and the MPE parameters are needed. The notebook already contains a variety of 37 scenarios which we have tested, which either use no buffer at all, pure random buffers, pure MPE buffers, pure Goldilocks buffers or mixed buffers.
The results (task 1/2 test accuracies after each epoch) will be saved as pickle files in the automatically created sub-folder “raw_result_data”, so further analysis can be performed on the data without having to redo the resource-intensive training.
Each time the notebook is run, a unique artificial experiment ID will be generated and used for all the saved pickle files. This is to allow the repeated and/or parallel execution of the notebook without overriding prior results (the file names contain the experiment ID and are therefore unique). Please note that the file names of the pickle files encode the scenario parameters (e.g. buffer_pct) together with the experiment ID, so each file can easily be identified.

# task_1_task_2_plotter.ipynb
This notebook is used to plot the results saved by the above notebook. In order to be able to perform a large variety of visualizations which were essential for the evaluation of the results, the notebook can be instructed to use a subset of all the scenarios, by using the filters in the variable “filter”, which is a dictionary, where for each scenario parameter (e.g. buffer_pct”) a list of allowed values can be set (if the list is empty, all values are allowed). The notebook will then generate one plot per distinct set of scenario parameters (i.e. all pickle files with the same file name except for the experiment ID). These plots are saved in the automatically created sub-folder “task_1_task_2_plots”. Depending if the variable “average_plots” is set to True or False, the notebook will average the data from all experiments with identical scenario parameters or just display the data from the first file it finds, as a sample.

# buffer-overlaps.ipynb
This notebook is used to analyze the overlaps of the chosen buffers in case of mixed (i.e. Goldilocks and MPE) buffers. It uses the data from the pickle files containing the accuracies, so no further training is needed. The idea is to find out for each possible buffer size, how many samples were contained in both the Goldilocks and the MPE buffers, which went into the mixed buffer. Please note that a mixed buffer of e.g. 40% buffer size implies concatenating a 20% pure Goldilocks buffer and a 20% pure MPE buffer, i.e. samples contained in both pure buffers will appear twice in the mixed buffer. This notebooks shows for each possible buffer size, what fraction of the mixed buffer consists of samples which were originally selected by both pure buffers. The idea was to figure out, how related the selections were. The dotted straight line shows the theoretical buffer overlap for the case where the buffer selections were totally unrelated. The more the selections are related, the more the overlap will be different from the straight line. The results clearly show, that the selections are indeed somehow related. The plots are saved as PNGs in the automatically created sub-folder “buffer_overlaps_results”.

# task_1_task_2_buffer_comparison_plotter.ipynb
This notebook visualizes the relation between the evolutions of the accuracies for the 1) pure Goldilocks buffer, 2) pure MPE buffer, and 3) mixed buffer, over all epochs of both task 1 and task 2 training, individually for each experiment. These plots are created for all experiments that can be found in the pickle files. The plots are then saved as PNGs in the automatically created sub-folder “task_1_task_2_buffer_comparison_plots”.

# task_1_task_2_final_accuracy_stats.ipynb
This notebook performs a statistical analysis of the final testing accuracies after the very last epoch, i.e after the end of the training procedure. The means and the standard deviations of the testing accuracies for task1 samples and task2 samples are calculated over all experiments (we performed 10) which could be found in the pickle files in “raw_result_data”, grouped by their scenario parameters. The results are saved in a CSV file in the automatically created sub-folder “task_1_task_2_final_accuracy_stats”, so the results can be integrated easily into a LaTeX file.

# kendalls_tau.ipynb
This notebook performs a comparison of the sample rankings used by the Goldilocks buffers and by the MPE buffers, by calculating Kendall’s Tau for the rankings, together with its p-value to see if the Null-Hypothesis (of the rankings being unrelated, i.e. Tau=0) can be rejected. This metric has been calculated in both directions, i.e. highest learning speed vs. highest sensitivity, and highest learning speed vs. lowest sensitivity. The Tau was very close to 0 and according to the high p-value, the null hypothesis cannot be rejected, i.e. according to the Kendall’s Tau metric, the possibility that the rankings are unrelated cannot be dicarded.